{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a title=\"Activity Recognition\" href=\"https://github.com/hisroar/NNonHexiwear/tree/master/HAR_TF_Example\" > DNNs for Human Activity Recognition</a>\n",
    "\n",
    "Based off of <a href=\"https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition\" > DNNs for Human Activity Recognition</a>.\n",
    "\n",
    "Human Activity Recognition (HAR) using smartphones dataset and an LSTM RNN. Classifying the type of movement amongst six categories:\n",
    "- WALKING,\n",
    "- WALKING_UPSTAIRS,\n",
    "- WALKING_DOWNSTAIRS,\n",
    "- SITTING,\n",
    "- STANDING,\n",
    "- LAYING.\n",
    "\n",
    "Data can be fed directly into the neural network who acts like a black box, modeling the problem correctly. [Other research](https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.names) on the activity recognition dataset can use a big amount of feature engineering, which is rather a signal processing approach combined with classical data science techniques. The approach here is rather very simple in terms of how much was the data preprocessed. \n",
    "\n",
    "Let's use Google's neat Deep Learning library, TensorFlow, demonstrating the usage of a DNN, a type of Artificial Neural Network that can process sequential data / time series. \n",
    "\n",
    "## Video dataset overview\n",
    "\n",
    "Follow this link to see a video of the 6 activities recorded in the experiment with one of the participants:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <a href=\"http://www.youtube.com/watch?feature=player_embedded&v=XOEN9W05_4A\n",
    "\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/XOEN9W05_4A/0.jpg\" \n",
    "alt=\"Video of the experiment\" width=\"400\" height=\"300\" border=\"10\" /></a>\n",
    "  <a href=\"https://youtu.be/XOEN9W05_4A\"><center>[Watch video]</center></a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Includes\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf  # Version 1.0.0 (some previous versions are used in past commits)\n",
    "from sklearn import metrics\n",
    "\n",
    "from tensorflow.python.framework.graph_util import remove_training_nodes\n",
    "from tensorflow.python.framework import graph_util as gu\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful Constants\n",
    "\n",
    "# Those are separate normalised input features for the neural network\n",
    "INPUT_SIGNAL_TYPES = [\n",
    "    \"body_acc_x_\",\n",
    "    \"body_acc_y_\",\n",
    "    \"body_acc_z_\",\n",
    "    \"body_gyro_x_\",\n",
    "    \"body_gyro_y_\",\n",
    "    \"body_gyro_z_\",\n",
    "#     \"total_acc_x_\",\n",
    "#     \"total_acc_y_\",\n",
    "#     \"total_acc_z_\"\n",
    "]\n",
    "n_input = len(INPUT_SIGNAL_TYPES)\n",
    "\n",
    "# Output classes to learn how to classify\n",
    "LABELS = [\n",
    "    \"WALKING\", \n",
    "    \"WALKING_UPSTAIRS\", \n",
    "    \"WALKING_DOWNSTAIRS\", \n",
    "    \"SITTING\", \n",
    "    \"STANDING\", \n",
    "    \"LAYING\"\n",
    "] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start by downloading the data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Linux bash commands start with a \"!\" inside those \"ipython notebook\" cells\n",
    "\n",
    "DATA_PATH = \"data/\"\n",
    "\n",
    "#!pwd && ls\n",
    "os.chdir(DATA_PATH)\n",
    "#!pwd && ls\n",
    "\n",
    "!python download_dataset.py\n",
    "\n",
    "#!pwd && ls\n",
    "os.chdir(\"..\")\n",
    "#!pwd && ls\n",
    "\n",
    "DATASET_PATH = DATA_PATH + \"UCI HAR Dataset/\"\n",
    "print(\"\\n\" + \"Dataset is now located at: \" + DATASET_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = \"train/\"\n",
    "TEST = \"test/\"\n",
    "\n",
    "\n",
    "# Load \"X\" (the neural network's training and testing inputs)\n",
    "\n",
    "def load_X(X_signals_paths):\n",
    "    X_signals = []\n",
    "    \n",
    "    for signal_type_path in X_signals_paths:\n",
    "        file = open(signal_type_path, 'r')\n",
    "        # Read dataset from disk, dealing with text files' syntax\n",
    "        X_signals.append(\n",
    "            [np.array(serie, dtype=np.float32) for serie in [\n",
    "                row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "            ]]\n",
    "        )\n",
    "        file.close()\n",
    "    \n",
    "    return np.transpose(np.array(X_signals), (1, 2, 0))\n",
    "\n",
    "X_train_signals_paths = [\n",
    "    DATASET_PATH + TRAIN + \"Inertial Signals/\" + signal + \"train.txt\" for signal in INPUT_SIGNAL_TYPES\n",
    "]\n",
    "X_test_signals_paths = [\n",
    "    DATASET_PATH + TEST + \"Inertial Signals/\" + signal + \"test.txt\" for signal in INPUT_SIGNAL_TYPES\n",
    "]\n",
    "\n",
    "X_train = load_X(X_train_signals_paths)\n",
    "X_test = load_X(X_test_signals_paths)\n",
    "\n",
    "# flatten the data to use in DNN\n",
    "X_train = np.reshape(X_train, [-1, 128 * n_input])\n",
    "X_test = np.reshape(X_test, [-1, 128 * n_input])\n",
    "\n",
    "# Load \"y\" (the neural network's training and testing outputs)\n",
    "\n",
    "def load_y(y_path):\n",
    "    file = open(y_path, 'r')\n",
    "    # Read dataset from disk, dealing with text file's syntax\n",
    "    y_ = np.array(\n",
    "        [elem for elem in [\n",
    "            row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "        ]], \n",
    "        dtype=np.int32\n",
    "    )\n",
    "    file.close()\n",
    "    \n",
    "    # Substract 1 to each output class for friendly 0-based indexing \n",
    "    return y_ - 1\n",
    "\n",
    "y_train_path = DATASET_PATH + TRAIN + \"y_train.txt\"\n",
    "y_test_path = DATASET_PATH + TEST + \"y_test.txt\"\n",
    "\n",
    "y_train = load_y(y_train_path)\n",
    "y_test = load_y(y_test_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Parameters:\n",
    "\n",
    "Here are some core parameter definitions for the training. \n",
    "\n",
    "For example, the whole neural network's structure could be summarised by enumerating those parameters and the fact that two LSTM are used one on top of another (stacked) output-to-input as hidden layers through time steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Data \n",
    "\n",
    "training_data_count = len(X_train)  # 7352 training series (with 50% overlap between each serie)\n",
    "test_data_count = len(X_test)  # 2947 testing series\n",
    "n_steps = 128#len(X_train[0])  # 128 timesteps per series\n",
    "\n",
    "\n",
    "# DNN's internal structure\n",
    "\n",
    "n_classes = 6 # Total classes (should go up, or should go down)\n",
    "\n",
    "\n",
    "# Training \n",
    "\n",
    "learning_rate = 0.0025\n",
    "lambda_loss_amount = 0.0015\n",
    "training_iters = 48000000#training_data_count * 10000  # Loop 300 times on the dataset\n",
    "batch_size = 1500\n",
    "display_iter = 1000000  # To show test set accuracy during training\n",
    "\n",
    "\n",
    "# Some debugging info\n",
    "\n",
    "print(\"Some useful info to get an insight on dataset's shape and normalisation:\")\n",
    "print(\"(X shape, y shape, every X's mean, every X's standard deviation)\")\n",
    "print(X_test.shape, y_test.shape, np.mean(X_test), np.std(X_test))\n",
    "print(\"The dataset is therefore properly normalised, as expected, but not yet one-hot encoded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The following code was left in because I got it to work with the modified dataset. However, uTensor\n",
    "#       doesn't support LSTM/RNN, nor does it support reshaping as of March 2019.\n",
    "\n",
    "#def LSTM_RNN(_X, _weights, _biases):\n",
    "#     # Function returns a tensorflow LSTM (RNN) artificial neural network from given parameters. \n",
    "#     # Moreover, two LSTM cells are stacked which adds deepness to the neural network. \n",
    "#     # Note, some code of this notebook is inspired from an slightly different \n",
    "#     # RNN architecture used on another dataset, some of the credits goes to \n",
    "#     # \"aymericdamien\" under the MIT license.\n",
    "\n",
    "#     # (NOTE: This step could be greatly optimised by shaping the dataset once\n",
    "#     # input shape: (batch_size, n_steps, n_input)\n",
    "#     _X = tf.transpose(_X, [1, 0, 2])  # permute n_steps and batch_size\n",
    "#     # Reshape to prepare input to hidden activation\n",
    "#     _X = tf.reshape(_X, [-1, n_input]) \n",
    "#     # new shape: (n_steps*batch_size, n_input)\n",
    "    \n",
    "#     # ReLU activation, thanks to Yu Zhao for adding this improvement here:\n",
    "#     _X = tf.nn.relu(tf.matmul(_X, _weights['hidden']) + _biases['hidden'])\n",
    "#     # Split data because rnn cell needs a list of inputs for the RNN inner loop\n",
    "#     _X = tf.split(_X, n_steps, 0) \n",
    "#     # new shape: n_steps * (batch_size, n_hidden)\n",
    "\n",
    "#     # Define two stacked LSTM cells (two recurrent layers deep) with tensorflow\n",
    "#     lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "#     lstm_cell_2 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "#     lstm_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell_1, lstm_cell_2], state_is_tuple=True)\n",
    "#     # Get LSTM cell output\n",
    "#     outputs, states = tf.contrib.rnn.static_rnn(lstm_cells, _X, dtype=tf.float32)\n",
    "\n",
    "#     # Get last time step's output feature for a \"many-to-one\" style classifier, \n",
    "#     # as in the image describing RNNs at the top of this page\n",
    "#     lstm_last_output = outputs[-1]\n",
    "    \n",
    "#     # Linear activation\n",
    "#     logits = tf.matmul(lstm_last_output, _weights['out']) + _biases['out']\n",
    "#     y_pred = tf.argmax(logits, 1, name=\"y_pred\")\n",
    "    \n",
    "#     return logits, y_pred\n",
    "\n",
    "# helper functions for defining NN variables\n",
    "def weight_variable(shape, name):\n",
    "    # weight_variable generates a weight variable of a given shape.\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial, name)\n",
    "\n",
    "def bias_variable(shape, name):\n",
    "    # bias_variable generates a bias variable of a given shape.\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial, name)\n",
    "\n",
    "# Fully connected NN\n",
    "# This can be simplified to 2 layers to save space\n",
    "def DEEP_NN(x):\n",
    "    # flattened input to 500 neurons\n",
    "    W_fc1 = weight_variable([n_steps * n_input, 500], name='W_fc1')\n",
    "    b_fc1 = bias_variable([500], name='b_fc1')\n",
    "    a_fc1 = tf.add(tf.matmul(x, W_fc1), b_fc1, name=\"zscore\")\n",
    "    h_fc1 = tf.nn.relu(a_fc1)\n",
    "    layer1 = tf.nn.dropout(h_fc1, 0.30)\n",
    "\n",
    "    # 500 input to 300 output\n",
    "    W_fc2 = weight_variable([500, 300], name='W_fc2')\n",
    "    b_fc2 = bias_variable([300], name='b_fc2')\n",
    "    a_fc2 = tf.add(tf.matmul(layer1, W_fc2), b_fc2, name=\"zscore\")\n",
    "    h_fc2 = tf.nn.relu(a_fc2)\n",
    "    layer2 = tf.nn.dropout(h_fc2, 0.30)\n",
    "    \n",
    "    # 300 input to 300 output\n",
    "    W_fc3 = weight_variable([300, 300], name='W_fc3')\n",
    "    b_fc3 = bias_variable([300], name='b_fc3')\n",
    "    a_fc3 = tf.add(tf.matmul(layer2, W_fc3), b_fc3, name=\"zscore\")\n",
    "    h_fc3 = tf.nn.relu(a_fc3)\n",
    "    layer3 = tf.nn.dropout(h_fc3, 0.50)\n",
    "    \n",
    "    # 300 input to 50 output\n",
    "    W_fc4 = weight_variable([300, 50], name='W_fc4')\n",
    "    b_fc4 = bias_variable([50], name='b_fc4')\n",
    "    a_fc4 = tf.add(tf.matmul(layer3, W_fc4), b_fc4, name=\"zscore\")\n",
    "    h_fc4 = tf.nn.relu(a_fc4)\n",
    "    layer4 = tf.nn.dropout(h_fc4, 0.50)\n",
    "\n",
    "    # 50 input to n_classes output\n",
    "    W_fc5 = weight_variable([50, n_classes], name='W_fc5')\n",
    "    b_fc5 = bias_variable([n_classes], name='b_fc5')\n",
    "    logits = tf.add(tf.matmul(layer4, W_fc5), b_fc5, name=\"logits\")\n",
    "    y_pred = tf.argmax(logits, 1, name='y_pred')\n",
    "\n",
    "    return y_pred, logits\n",
    "\n",
    "def extract_batch_size(_train, step, batch_size):\n",
    "    # Function to fetch a \"batch_size\" amount of data from \"(X|y)_train\" data. \n",
    "    \n",
    "    shape = list(_train.shape)\n",
    "    shape[0] = batch_size\n",
    "    batch_s = np.empty(shape)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Loop index\n",
    "        index = ((step-1)*batch_size + i) % len(_train)\n",
    "        batch_s[i] = _train[index] \n",
    "\n",
    "    return batch_s\n",
    "\n",
    "\n",
    "def one_hot(y_, n_classes=n_classes):\n",
    "    # Function to encode neural one-hot output labels from number indexes \n",
    "    # e.g.: \n",
    "    # one_hot(y_=[[5], [0], [3]], n_classes=6):\n",
    "    #     return [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n",
    "    \n",
    "    y_ = y_.reshape(len(y_))\n",
    "    return np.eye(n_classes)[np.array(y_, dtype=np.int32)]  # Returns FLOATS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Graph input/output\n",
    "x = tf.placeholder(tf.float32, [None, n_steps * n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Uncomment LSTM code and this code to get LSTM running\n",
    "# pred, y_pred = LSTM_RNN(x, weights, biases)\n",
    "y_pred, logits = DEEP_NN(x)\n",
    "\n",
    "with tf.name_scope(\"Loss\"):\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y,\n",
    "                                                               logits=logits)\n",
    "    cost = tf.reduce_mean(cross_entropy, name=\"cross_entropy_loss\")\n",
    "    optimizer = tf.train.AdamOptimizer(1e-4).minimize(cost, name=\"train_step\")\n",
    "\n",
    "with tf.name_scope(\"Prediction\"):\n",
    "    correct_pred = tf.equal(y_pred, tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name=\"accuracy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hooray, now train the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To keep track of training's performance\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "# Launch the graph\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Perform Training steps with \"batch_size\" amount of example data at each loop\n",
    "step = 1\n",
    "while step * batch_size <= training_iters:\n",
    "    batch_xs =         extract_batch_size(X_train, step, batch_size)\n",
    "    batch_ys = one_hot(extract_batch_size(y_train, step, batch_size))\n",
    "\n",
    "    # Fit training using batch data\n",
    "    _, loss, acc = sess.run(\n",
    "        [optimizer, cost, accuracy],\n",
    "        feed_dict={\n",
    "            x: batch_xs, \n",
    "            y: batch_ys\n",
    "        }\n",
    "    )\n",
    "    train_losses.append(loss)\n",
    "    train_accuracies.append(acc)\n",
    "    \n",
    "    # Evaluate network only at some steps for faster training: \n",
    "    if (step*batch_size % display_iter == 0) or (step == 1) or (step * batch_size > training_iters):\n",
    "        \n",
    "        # To not spam console, show training accuracy/loss in this \"if\"\n",
    "        print(\"Training iter #\" + str(step*batch_size) + \\\n",
    "              \":   Batch Loss = \" + \"{:.6f}\".format(loss) + \\\n",
    "              \", Accuracy = {}\".format(acc))\n",
    "        \n",
    "        # Evaluation on the test set (no learning made here - just evaluation for diagnosis)\n",
    "        loss, acc = sess.run(\n",
    "            [cost, accuracy], \n",
    "            feed_dict={\n",
    "                x: X_test,\n",
    "                y: one_hot(y_test)\n",
    "            }\n",
    "        )\n",
    "        test_losses.append(loss)\n",
    "        test_accuracies.append(acc)\n",
    "        print(\"PERFORMANCE ON TEST SET: \" + \\\n",
    "              \"Batch Loss = {}\".format(loss) + \\\n",
    "              \", Accuracy = {}\".format(acc))\n",
    "\n",
    "    step += 1\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "# Accuracy for test data\n",
    "\n",
    "one_hot_predictions, accuracy, final_loss = sess.run(\n",
    "    [logits, accuracy, cost],\n",
    "    feed_dict={\n",
    "        x: X_test,\n",
    "        y: one_hot(y_test)\n",
    "    }\n",
    ")\n",
    "\n",
    "test_losses.append(final_loss)\n",
    "test_accuracies.append(accuracy)\n",
    "\n",
    "print(\"FINAL RESULT: \" + \\\n",
    "      \"Batch Loss = {}\".format(final_loss) + \\\n",
    "      \", Accuracy = {}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training is good, but having visual insight is even better:\n",
    "\n",
    "Okay, let's plot this simply in the notebook for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Inline plots: )\n",
    "%matplotlib inline\n",
    "\n",
    "font = {\n",
    "    'family' : 'Bitstream Vera Sans',\n",
    "    'weight' : 'bold',\n",
    "    'size'   : 18\n",
    "}\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "width = 12\n",
    "height = 12\n",
    "plt.figure(figsize=(width, height))\n",
    "\n",
    "indep_train_axis = np.array(range(batch_size, (len(train_losses)+1)*batch_size, batch_size))\n",
    "plt.plot(indep_train_axis, np.array(train_losses),     \"b--\", label=\"Train losses\")\n",
    "plt.plot(indep_train_axis, np.array(train_accuracies), \"g--\", label=\"Train accuracies\")\n",
    "\n",
    "indep_test_axis = np.append(\n",
    "    np.array(range(batch_size, len(test_losses)*display_iter, display_iter)[:-1]),\n",
    "    [training_iters]\n",
    ")\n",
    "plt.plot(indep_test_axis, np.array(test_losses),     \"b-\", label=\"Test losses\")\n",
    "plt.plot(indep_test_axis, np.array(test_accuracies), \"g-\", label=\"Test accuracies\")\n",
    "\n",
    "plt.title(\"Training session's progress over iterations\")\n",
    "plt.legend(loc='upper right', shadow=True)\n",
    "plt.ylabel('Training Progress (Loss or Accuracy values)')\n",
    "plt.xlabel('Training iteration')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And finally, the multi-class confusion matrix and metrics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "\n",
    "predictions = one_hot_predictions.argmax(1)\n",
    "\n",
    "print(\"Testing Accuracy: {}%\".format(100*accuracy))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Precision: {}%\".format(100*metrics.precision_score(y_test, predictions, average=\"weighted\")))\n",
    "print(\"Recall: {}%\".format(100*metrics.recall_score(y_test, predictions, average=\"weighted\")))\n",
    "print(\"f1_score: {}%\".format(100*metrics.f1_score(y_test, predictions, average=\"weighted\")))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Confusion Matrix:\")\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, predictions)\n",
    "print(confusion_matrix)\n",
    "normalised_confusion_matrix = np.array(confusion_matrix, dtype=np.float32)/np.sum(confusion_matrix)*100\n",
    "\n",
    "print(\"\")\n",
    "print(\"Confusion matrix (normalised to % of total test data):\")\n",
    "print(normalised_confusion_matrix)\n",
    "print(\"Note: training and testing data is not equally distributed amongst classes, \")\n",
    "print(\"so it is normal that more than a 6th of the data is correctly classifier in the last category.\")\n",
    "\n",
    "# Plot Results: \n",
    "width = 12\n",
    "height = 12\n",
    "plt.figure(figsize=(width, height))\n",
    "plt.imshow(\n",
    "    normalised_confusion_matrix, \n",
    "    interpolation='nearest', \n",
    "    cmap=plt.cm.rainbow\n",
    ")\n",
    "plt.title(\"Confusion matrix \\n(normalised to % of total test data)\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(n_classes)\n",
    "plt.xticks(tick_marks, LABELS, rotation=90)\n",
    "plt.yticks(tick_marks, LABELS)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the DNN as a .pb file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the graph\n",
    "saver.save(sess, \"./chkps/lstm_model\")\n",
    "out_nodes = [y_pred.op.name]\n",
    "print(out_nodes)\n",
    "\n",
    "# Remove unnecessary training nodes\n",
    "sub_graph_def = remove_training_nodes(sess.graph_def)\n",
    "\n",
    "# Freeze constants\n",
    "sub_graph_def = gu.convert_variables_to_constants(sess, sub_graph_def, out_nodes)\n",
    "\n",
    "# Save graph to PB file\n",
    "\n",
    "graph_path = tf.train.write_graph(sub_graph_def,\n",
    "                                  \"./output\",\n",
    "                                  \"dnn6_4.pb\",\n",
    "                                  as_text=False)\n",
    "\n",
    "print('written graph to: %s' % graph_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "The [dataset](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones) can be found on the UCI Machine Learning Repository: \n",
    "\n",
    "> Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz. A Public Domain Dataset for Human Activity Recognition Using Smartphones. 21th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, ESANN 2013. Bruges, Belgium 24-26 April 2013.\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
